{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of Aviation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Parse XML Aviation Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Documents/Spring_2020/data-scientist-exercise02/data/AviationData.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment and run to get column names\n",
    "#tree = et.parse(file)\n",
    "#root = tree.getroot()\n",
    "#for elem in root:\n",
    "    #for subelem in elem:\n",
    "        #print(subelem.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['EventId','InvestigationType','AccidentNumber','EventDate','Location','Country','Latitude','Longitude','AirportCode',\n",
    "    'AirportName','InjurySeverity','AircraftDamage','AircraftCategory','RegistrationNumber','Make','Model','AmateurBuilt','NumberOfEngines',\n",
    "    'EngineType','FARDescription','Schedule','PurposeOfFlight','AirCarrier','TotalFatalInjuries','TotalSeriousInjuries','TotalMinorInjuries',\n",
    "    'TotalUninjured','WeatherCondition','BroadPhaseOfFlight','ReportStatus','PublicationDate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XML(xml_file, df_cols): \n",
    "    \n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    for node in xroot:\n",
    "        for subnode in node:\n",
    "            res = []\n",
    "            for i in range(len(df_cols)):\n",
    "                res.append(subnode.attrib.get(df_cols[i]))\n",
    "            rows.append({df_cols[i]: res[i] for i, _ in enumerate(df_cols)})\n",
    "\n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_xml = parse_XML(file,col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Merge JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(\"Documents/Spring_2020/data-scientist-exercise02/data/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFilesDict = {v:k for v, k in enumerate(file_list, 1)}\n",
    "\n",
    "data = []\n",
    "\n",
    "for k,v in allFilesDict.items():\n",
    "    with open(v, 'r') as d:\n",
    "        jdata = json.load(d)\n",
    "        if jdata:\n",
    "            data.append(jdata)\n",
    "\n",
    "data_list = []\n",
    "for i in range(0,len(data)):\n",
    "    data_1_file = data[i]\n",
    "    data_1_file = data_1_file['data']\n",
    "    df = pd.DataFrame(data_1_file)\n",
    "    data_list.append(df)\n",
    "\n",
    "json_data = pd.concat(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.merge(parsed_xml, json_data, on='EventId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore, Clean, and Enrich Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data.replace('',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    61829\n",
       "1    15428\n",
       "Name: FatalIndicator, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create fatal indicator column to indicate if accident was deadly\n",
    "combined_data['TotalFatalInjuries'] = combined_data['TotalFatalInjuries'].astype(float)\n",
    "combined_data['FatalIndicator'] = combined_data['TotalFatalInjuries'].apply(lambda x: 1 if x > 0 else 0)\n",
    "combined_data['FatalIndicator'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Accidents with no event date- only 4 observations\n",
    "#Create fields for year, month, day, and day of week\n",
    "combined_data = combined_data.dropna(subset= ['EventDate'])\n",
    "combined_data['Year'] = pd.DatetimeIndex(combined_data['EventDate']).year.astype(int)\n",
    "combined_data['Month'] = pd.DatetimeIndex(combined_data['EventDate']).month.astype(int)\n",
    "combined_data['Day'] = pd.DatetimeIndex(combined_data['EventDate']).day.astype(int)\n",
    "combined_data['WeekDay'] = pd.to_datetime(combined_data['EventDate']).apply(lambda x: x.weekday()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv('Downloads/combined_aviation_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
